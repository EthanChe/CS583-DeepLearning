{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C-ChwRf5c_k"
      },
      "source": [
        "# Assignment 2: Build a CNN for image recognition.\n",
        "\n",
        "## Due Date:  March 29, 11:59PM\n",
        "\n",
        "### Name: Ethan Che\n",
        "\n",
        "I pledge my honor that I have abided by the Stevens Honor System.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ij5Wwjn5c_6"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "1. In this assignment, you will build Convolutional Neural Network to classify CIFAR-10 Images.\n",
        "2. You can directly load dataset from many deep learning packages.\n",
        "3. You can use any deep learning packages such as pytorch, keras or tensorflow for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvKG-qe-5dAB"
      },
      "source": [
        "## Requirements:\n",
        "\n",
        "1. You need to load cifar 10 data and split the entire training dataset into training and validation.\n",
        "2. You will implement a CNN model to classify cifar 10 images with provided structure.\n",
        "3. You need to plot the training and validation accuracy or loss obtained from above step.\n",
        "4. Then you can use tuned parameters to train using the entire training dataset.\n",
        "5. You should report the testing accuracy using the model with complete data.\n",
        "6. You may try to change the structure (e.g, add BN layer or dropout layer,...) and analyze your findings.\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Zhpe1_5dAE"
      },
      "source": [
        "## Batch Normalization (BN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwljr4Y55dAF"
      },
      "source": [
        "### Background:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcpWFAjw5dAF"
      },
      "source": [
        "- Batch Normalization is a technique to speed up training and help make the model more stable.\n",
        "- In simple words, batch normalization is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
        "\n",
        "- For more detailed information, you may refer to the original paper: https://arxiv.org/pdf/1502.03167.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at65rbVv5dAG"
      },
      "source": [
        "### BN Algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLQ5K8H65dAG"
      },
      "source": [
        "- Input: Values of $x$ over a mini-batch: $\\mathbf{B}$ = $\\{x_1,..., x_m\\};$\n",
        "- Output: $\\{y_i = BN_{\\gamma,\\beta}(x_i)\\}$, $\\gamma, \\beta$ are learnable parameters\n",
        "\n",
        "Normalization of the Input:\n",
        "$$\\mu_{\\mathbf{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$$\n",
        "$$\\sigma_{\\mathbf{B}}^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathbf{B}})^2$$\n",
        "$$\\hat{x_i} = \\frac{x_i - \\mu_{\\mathbf{B}}}{\\sqrt{\\sigma_{\\mathbf{B}}}^2 + \\epsilon}$$\n",
        "Re-scaling and Offsetting:\n",
        "$$y_i = \\gamma \\hat{x_i} + \\beta = BN_{\\gamma,\\beta}(x_i)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17yhGz5V5dAL"
      },
      "source": [
        "### Advantages of BN:\n",
        "1. Improves gradient flow through the network.\n",
        "2. Allows use of saturating nonlinearities and higher learning rates.\n",
        "3. Makes weights easier to initialize.\n",
        "4. Act as a form of regularization and may reduce the need for dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsDrbizi5dAM"
      },
      "source": [
        "### Implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjrBCeW55dAN"
      },
      "source": [
        "- The batch normalization layer has already been implemented in many packages. You may simply call the function to build the layer. For example: torch.nn.BatchNorm2d() using pytroch package, keras.layers.BatchNormalization() using keras package.\n",
        "- The location of BN layer: Please make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuqKjkW_5dAN"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95n--0Vo5dAN"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-d-dNKE5dAO",
        "outputId": "4f213798-c876-43b3-eac4-7917841edac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Load Cifar-10 Data\n",
        "# This is just an example, you may load dataset from other packages.\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "### If you can not load keras dataset, un-comment these two lines.\n",
        "#import ssl\n",
        "#ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3F_bRm85dAQ"
      },
      "source": [
        "### 1.2. One-hot encode the labels (5 points)\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Implement a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pm6loGcO5dAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3a679e-3bb3-4b70-8e57-e3b618616f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    hot = []\n",
        "    for x in y: #For every element in input array\n",
        "        temp = []\n",
        "        for i in range(10):\n",
        "            if (i == x[0]):\n",
        "                temp.append(1)\n",
        "            else:\n",
        "                temp.append(0)\n",
        "        hot.append(temp)\n",
        "    return hot\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(np.shape(y_train_vec)))\n",
        "print('Shape of y_test_vec: ' + str(np.shape(y_test_vec)))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIFKdbT5dAR"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLdcno2s5dAR"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets (5 points)\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets: \n",
        "* a training set containing 40K samples: x_tr, y_tr\n",
        "* a validation set containing 10K samples: x_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KlE5VV_h5dAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3125cb57-8757-4227-df09-54740657cd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 1)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.20, random_state=18)\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))\n",
        "\n",
        "y_tr_vec = to_one_hot(y_tr)\n",
        "y_val_vec = to_one_hot(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpigR6Eb5dAS"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters (50 points)\n",
        "\n",
        "- Build a convolutional neural network model using the below structure:\n",
        "\n",
        "- It should have a structure of: Conv - ReLU - Max Pool - ConV - ReLU - Max Pool - Dense - ReLU - Dense - Softmax\n",
        "\n",
        "- In the graph 3@32x32 means the dimension of input image, 32@30x30 means it has 32 filters and the dimension now becomes 30x30 after the convolution.\n",
        "- All convolutional layers (Conv) should have stride = 1 and no padding.\n",
        "- Max Pooling has a pool size of 2 by 2.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSQqnaVa5dAT"
      },
      "source": [
        "<img src=\"network.PNG\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NfsABQa5dAT"
      },
      "source": [
        "- You may use the validation data to tune the hyper-parameters (e.g., learning rate, and optimization algorithm)\n",
        "- Do NOT use test data for hyper-parameter tuning!!!\n",
        "- Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YTlyiJ1s5dAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0e2c88-31f6-4d18-cf6f-727bed612250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 626,378\n",
            "Trainable params: 626,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape = (32, 32, 3))) #3-by-3 convolution to get from 32x32 to 30x30\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (4,4), activation='relu', input_shape=(15,15,32)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0y7uMqoV5dAX"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "\n",
        "model.compile(tf.keras.optimizers.RMSprop(learning_rate=0.0008),loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZnCY63de5dAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be911ee8-e989-412a-be84-643b4da9f21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "157/157 [==============================] - 13s 13ms/step - loss: 4.3764 - accuracy: 0.2267 - val_loss: 1.9562 - val_accuracy: 0.3164\n",
            "Epoch 2/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 1.7716 - accuracy: 0.3929 - val_loss: 1.5633 - val_accuracy: 0.4527\n",
            "Epoch 3/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.4096 - accuracy: 0.5075 - val_loss: 1.8283 - val_accuracy: 0.4127\n",
            "Epoch 4/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.1843 - accuracy: 0.5919 - val_loss: 1.6482 - val_accuracy: 0.4434\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.9783 - accuracy: 0.6634 - val_loss: 1.3716 - val_accuracy: 0.5530\n",
            "Epoch 6/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.8014 - accuracy: 0.7252 - val_loss: 1.3167 - val_accuracy: 0.5861\n",
            "Epoch 7/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6437 - accuracy: 0.7811 - val_loss: 1.5422 - val_accuracy: 0.5726\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.5101 - accuracy: 0.8243 - val_loss: 1.5000 - val_accuracy: 0.6090\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.3956 - accuracy: 0.8678 - val_loss: 1.7892 - val_accuracy: 0.5766\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.3107 - accuracy: 0.8969 - val_loss: 1.9845 - val_accuracy: 0.5911\n",
            "Epoch 11/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.2401 - accuracy: 0.9211 - val_loss: 2.1061 - val_accuracy: 0.6175\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.2005 - accuracy: 0.9349 - val_loss: 2.1777 - val_accuracy: 0.6175\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.1683 - accuracy: 0.9459 - val_loss: 2.1886 - val_accuracy: 0.6234\n",
            "Epoch 14/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.1403 - accuracy: 0.9566 - val_loss: 2.3756 - val_accuracy: 0.6266\n",
            "Epoch 15/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.1283 - accuracy: 0.9605 - val_loss: 2.6134 - val_accuracy: 0.6067\n",
            "Epoch 16/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.1105 - accuracy: 0.9663 - val_loss: 3.2048 - val_accuracy: 0.5987\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.1065 - accuracy: 0.9678 - val_loss: 2.9656 - val_accuracy: 0.6121\n",
            "Epoch 18/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.1010 - accuracy: 0.9691 - val_loss: 2.9424 - val_accuracy: 0.6088\n",
            "Epoch 19/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0870 - accuracy: 0.9722 - val_loss: 3.2546 - val_accuracy: 0.6038\n",
            "Epoch 20/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.1000 - accuracy: 0.9719 - val_loss: 3.4852 - val_accuracy: 0.6243\n",
            "Epoch 21/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0821 - accuracy: 0.9761 - val_loss: 3.4185 - val_accuracy: 0.6319\n",
            "Epoch 22/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0865 - accuracy: 0.9762 - val_loss: 4.0866 - val_accuracy: 0.5848\n",
            "Epoch 23/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.0698 - accuracy: 0.9794 - val_loss: 3.9880 - val_accuracy: 0.6133\n",
            "Epoch 24/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0771 - accuracy: 0.9775 - val_loss: 3.7739 - val_accuracy: 0.6082\n",
            "Epoch 25/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.0814 - accuracy: 0.9786 - val_loss: 3.9432 - val_accuracy: 0.6235\n",
            "Epoch 26/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0759 - accuracy: 0.9788 - val_loss: 4.1415 - val_accuracy: 0.6187\n",
            "Epoch 27/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0694 - accuracy: 0.9799 - val_loss: 4.7983 - val_accuracy: 0.5785\n",
            "Epoch 28/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0699 - accuracy: 0.9809 - val_loss: 4.8216 - val_accuracy: 0.5956\n",
            "Epoch 29/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0728 - accuracy: 0.9815 - val_loss: 4.8840 - val_accuracy: 0.6092\n",
            "Epoch 30/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0685 - accuracy: 0.9822 - val_loss: 4.3440 - val_accuracy: 0.6271\n",
            "Epoch 31/50\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0654 - accuracy: 0.9827 - val_loss: 4.5957 - val_accuracy: 0.6168\n",
            "Epoch 32/50\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0693 - accuracy: 0.9832 - val_loss: 4.9080 - val_accuracy: 0.6148\n",
            "Epoch 33/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.0644 - accuracy: 0.9826 - val_loss: 4.8393 - val_accuracy: 0.6227\n",
            "Epoch 34/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.0588 - accuracy: 0.9838 - val_loss: 4.8619 - val_accuracy: 0.6028\n",
            "Epoch 35/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0567 - accuracy: 0.9845 - val_loss: 5.2595 - val_accuracy: 0.6178\n",
            "Epoch 36/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0658 - accuracy: 0.9839 - val_loss: 5.4478 - val_accuracy: 0.5919\n",
            "Epoch 37/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0619 - accuracy: 0.9846 - val_loss: 5.4541 - val_accuracy: 0.6101\n",
            "Epoch 38/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0615 - accuracy: 0.9844 - val_loss: 6.3574 - val_accuracy: 0.5834\n",
            "Epoch 39/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0635 - accuracy: 0.9842 - val_loss: 5.3727 - val_accuracy: 0.6168\n",
            "Epoch 40/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0597 - accuracy: 0.9852 - val_loss: 6.2335 - val_accuracy: 0.6260\n",
            "Epoch 41/50\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.0594 - accuracy: 0.9858 - val_loss: 6.1410 - val_accuracy: 0.5856\n",
            "Epoch 42/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0586 - accuracy: 0.9858 - val_loss: 5.6008 - val_accuracy: 0.6269\n",
            "Epoch 43/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0580 - accuracy: 0.9856 - val_loss: 5.4584 - val_accuracy: 0.5994\n",
            "Epoch 44/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0630 - accuracy: 0.9850 - val_loss: 5.7491 - val_accuracy: 0.6126\n",
            "Epoch 45/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0535 - accuracy: 0.9865 - val_loss: 7.1714 - val_accuracy: 0.6046\n",
            "Epoch 46/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0625 - accuracy: 0.9859 - val_loss: 6.4559 - val_accuracy: 0.6110\n",
            "Epoch 47/50\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0561 - accuracy: 0.9864 - val_loss: 7.0557 - val_accuracy: 0.5901\n",
            "Epoch 48/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0622 - accuracy: 0.9863 - val_loss: 6.4602 - val_accuracy: 0.6029\n",
            "Epoch 49/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0620 - accuracy: 0.9869 - val_loss: 6.6089 - val_accuracy: 0.6239\n",
            "Epoch 50/50\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.0560 - accuracy: 0.9868 - val_loss: 7.0286 - val_accuracy: 0.6109\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "history = model.fit(np.array(x_tr), np.array(y_tr_vec), batch_size=256, epochs=50, validation_data=(np.array(x_val), np.array(y_val_vec)))\n",
        "\n",
        "bsize = 256 #parameters to use\n",
        "lr = 0.0008\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cNfxNJX5dAX"
      },
      "source": [
        "## 3. Plot the training and validation loss curve versus epochs. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-S7xr-qF5dAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "7a4a7b4e-ea52-4dbf-d1ed-1a296867c895"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yUVfb48c8h9CrVQjFYAJWSQAAFC8V1UViqKAoqWCj6FcEV1LWv8hX9WfmuZREBC0qxgAjCKlJUbKGIlKgoAQNIXZqAlNzfH2cGE8hMJpN5MpmZ83695jWZZ8pzHw1nbs6991xxzmGMMSb+lIh2A4wxxnjDArwxxsQpC/DGGBOnLMAbY0ycsgBvjDFxqmS0G5BTjRo1XHJycrSbYYwxMWPJkiXbnXM183quWAX45ORk0tPTo90MY4yJGSKyPtBzlqIxxpg4ZQHeGGPilAV4Y4yJU8UqB5+Xw4cPk5WVxcGDB6PdFBOismXLUqdOHUqVKhXtphiT0Ip9gM/KyqJSpUokJycjItFujsmHc44dO3aQlZVF/fr1o90cYxJasU/RHDx4kOrVq1twjxEiQvXq1e0vLmOKgWIf4AEL7jHG/n8ZUzzERIA3xhhPrVwJ8+ZFuxURZwE+iB07dpCSkkJKSgqnnHIKtWvXPvb40KFDQd+bnp7O0KFD8z1HmzZtItLWBQsW0KVLl4h8ljEJ5957oUcP+OOPaLckouIuwE+aBMnJUKKE3k+aFP5nVa9eneXLl7N8+XIGDx7M8OHDjz0uXbo0R44cCfjetLQ0xowZk+85Fi9eHH4DjTGRsWYN7N0LCxZEuyUR5VmAF5GGIrI8x22PiAzz6nygwXzgQFi/HpzT+4EDCxfkj9e/f38GDx5M69atGTlyJN988w0XXHABqamptGnThh9++AHI3aN++OGHufHGG2nXrh1nnHFGrsBfsWLFY69v164dV155JY0aNaJv3774d9uaPXs2jRo1okWLFgwdOrRAPfW3336bJk2a0LhxY+6++24Ajh49Sv/+/WncuDFNmjTh2WefBWDMmDGce+65NG3alD59+hT+P5YxseDgQVi3Tn+ePj309y1bpoGmGPNsmqRz7gcgBUBEkoCNwPtenQ/gvvtg//7cx/bv1+N9+0buPFlZWSxevJikpCT27NnDZ599RsmSJfnkk0/4xz/+wbvvvnvCezIyMpg/fz579+6lYcOGDBky5IR54suWLWPVqlWcdtpptG3bli+++IK0tDQGDRrEokWLqF+/Ptdcc03I7dy0aRN33303S5YsoWrVqlx22WVMnz6dunXrsnHjRlauXAnArl27ABg9ejTr1q2jTJkyx44ZE/fWroXsbKhYEWbMgBde0BRAMB99BFdcAdOmwZVXFu78s2bBqlUwbBiULl24zzpOUaVoOgI/O+cCFsWJhA0bCnY8XL179yYpKQmA3bt307t3bxo3bszw4cNZtWpVnu/p3LkzZcqUoUaNGtSqVYstW7ac8JpWrVpRp04dSpQoQUpKCpmZmWRkZHDGGWccm1NekAD/7bff0q5dO2rWrEnJkiXp27cvixYt4owzzuCXX37h9ttvZ86cOVSuXBmApk2b0rdvX958801Kliz2SySMiYw1a/R+0CDYvBm+/Tb/94wfn/s+XEeOwF13wYQJ+X+phKGoAnwf4O28nhCRgSKSLiLp27ZtK9RJ6tUr2PFwVahQ4djPDzzwAO3bt2flypXMnDkz4PzvMmXKHPs5KSkpz/x9KK+JhKpVq/Ldd9/Rrl07Xn75ZW6++WYAZs2axW233cbSpUtp2bKlZ+c3pljJyAARuPNOSErKP02zcyd88AFUrgxz58LGjeGf+/XX9fyjRoEHnSrPA7yIlAa6AtPyet45N9Y5l+acS6tZM8+SxiEbNQrKl899rHx5Pe6V3bt3U7t2bQAmTpwY8c9v2LAhv/zyC5mZmQBMmTIl5Pe2atWKhQsXsn37do4ePcrbb7/NJZdcwvbt28nOzqZXr1489thjLF26lOzsbH799Vfat2/PE088we7du9m3b1/Er8eYYicjA04/HU47Ddq1yz/AT54Mhw7BuHGa2nnzzfDOe+AAPPQQtGqlM3g8UBQ9+MuBpc65E3MSEda3L4wdq/+vRPR+7NjI5t+PN3LkSO69915SU1M96fGWK1eOF198kU6dOtGiRQsqVapElSpV8nztvHnzqFOnzrFbZmYmo0ePpn379jRr1owWLVrQrVs3Nm7cSLt27UhJSaFfv348/vjjHD16lH79+tGkSRNSU1MZOnQoJ510UsSvx5hiJyMDGjXSn7t318cZGYFfP3EiNGsGvXtD27b6OJzB1hdfhKwsGD1aA5YXnHOe3oDJwIBQXtuiRQt3vNWrV59wLNHs3bvXOedcdna2GzJkiHvmmWei3KL82f83ExOOHnWufHnnhg3Txxs2OAfOjR6d9+tXrdLn/f8Gx43Tx199VbDz7trlXLVqzv31r+G33QdIdwFiqqc9eBGpAPwFeM/L88S7V155hZSUFM477zx2797NoEGDot0kY+JDVpZOtTvnHH1cty60aBE4TfPaa5or96cFeveGcuW0F18Q/+//aS7/8cfDbnooPA3wzrnfnXPVnXO7vTxPvPMvsFq9ejWTJk2i/PEDDcaY8PhTMf4UDWia5quvdEZNTkePar798suhVi09Vrky9OoFb7+tOfVQbN4Mzz4LffpAamrhryGIuFvJaowxIfNPkTw+wIPOlMnpk09g0ybo3z/38f79YfdunUMfisce00HaRx8Np8UFYgHeGJO4MjKgalXIOYPvvPPgzDNPTNNMnAjVqkHnzrmPt2+vqZ1Q0jRr1+rMj1tugbPOKmzr82UB3hiTuDIyNP+ecxaLiPbi582DPXv02K5dGvCvvRZyrFcBdIHSDTfAxx/nPyf+wQd1teoDD0T2OgKwAG+MSVw5p0jm1L07HD6sJQkApk7VmjU33JD35/Tvr3Pi33gj8LmWLdNc/bBhcOqphW56KCzA56N9+/bMnTs317HnnnuOIUOGBHxPu3btSE9PB+CKK67Is67Lww8/zFNPPRX03NOnT2f16tXHHj/44IN88sknBWl+nqy0sDFor/y33/IO8BdcoGkbf5rmtdc0ddOiRd6fdeaZcNFFWnIgrznx69ZpWqZqVRgxInLXkA8L8Pm45pprmDx5cq5jkydPDrkmzOzZs8NeMHR8gP/nP//JpZdeGtZnGWOOk9cMGr+kJOjaVQuBrVwJixdr7z3YgqT+/eHHH3UGjl92ti5oatJEnxs3DopwAaEF+HxceeWVzJo169gGH5mZmWzatImLLrqIIUOGkJaWxnnnncdDDz2U5/uTk5PZvn07AKNGjaJBgwZceOGFx8oKg85zb9myJc2aNaNXr17s37+fxYsX88EHHzBixAhSUlL4+eef6d+/P++88w6gq1ZTU1Np0qQJN954I3/4NipITk7moYceonnz5jRp0oSMYCvyjmOlhU1C8f/b8M+BP1737loj/uabNc/er1/wz+vdW2uj+Adb162Djh3httt0xevKldCzZ8SaH4rYKhk4bBgsXx7Zz0xJgeeeC/h0tWrVaNWqFR999BHdunVj8uTJXHXVVYgIo0aNolq1ahw9epSOHTuyYsUKmjZtmufnLFmyhMmTJ7N8+XKOHDlC8+bNaeH7c69nz57ccsstANx///28+uqr3H777XTt2pUuXbpw5XHlSA8ePEj//v2ZN28eDRo04Prrr+ell15i2DAtt1+jRg2WLl3Kiy++yFNPPcW4cePy/c9gpYVNwsnI0AHP5OS8n+/YESpUgK+/1rnv+eXNK1XSOfGTJ+uXxv33618C48bBjTd6V44gCOvBhyBnmiZnembq1Kk0b96c1NRUVq1alSudcrzPPvuMHj16UL58eSpXrkzXrl2PPbdy5UouuugimjRpwqRJkwKWHPb74YcfqF+/Pg0aNADghhtuYNGiRcee7+nrJbRo0eJYkbL8WGlhk3DWrIGzzw5cxbFcOejUSX8+fu57IAMG6Myb4cPhwgu1137TTVEJ7hBrPfggPW0vdevWjeHDh7N06VL2799PixYtWLduHU899RTffvstVatWpX///gFLBeenf//+TJ8+nWbNmjFx4kQWFHLbMH/Z4UiUHPaXFp47dy4vv/wyU6dOZfz48cyaNYtFixYxc+ZMRo0axffff2+B3sSWjAzNjQdz222wb5/m40NxySVw++2aGRgwIGqB3c968CGoWLEi7du358YbbzzWe9+zZw8VKlSgSpUqbNmyhY/806kCuPjii5k+fToHDhxg7969zJw589hze/fu5dRTT+Xw4cNMyrG/YKVKldi7d+8Jn9WwYUMyMzNZu3YtAG+88QaXXHJJoa7RSgubhHLoEPz8c+D8u1/79jBnDpQtG9rnligBY8ZELSVzPOtyheiaa66hR48ex1I1zZo1IzU1lUaNGlG3bl3atm0b9P3Nmzfn6quvplmzZtSqVYuWLVsee+7RRx+ldevW1KxZk9atWx8L6n369OGWW25hzJgxxwZXAcqWLcuECRPo3bs3R44coWXLlgwePLhA1+MvLew3bdq0Y6WFnXN07tyZbt268d133zFgwACys7MBcpUW3r17N845Ky1sTrR7t+atx4yBtLRot+ZEa9dqbZm8ZtDEEXHFaNPYtLQ0558/7rdmzRrOye9b1hQ79v8twb33ng44jhwJTzwR7dacyN++9PTAc9tjhIgscc7l+S1qKRpjTOQtXKj3ixdHtx2B+KdINmwY3XZ4zAK8MSby/AH+22813x2K77/XioyHD3vXLr+MDKhTBypW9P5cURQTAb44pZFM/uz/V4LbuRNWrNCZJH/8AUuXhva+m2/WxUX16mlRrqws79q4Zk3+A6xxoNgH+LJly7Jjxw4LGjHCOceOHTsoG+qsAxN/PvtM67Hce68+DiVNs2uX5sN79dKc+GOP6QKknj21DrtvkD8inAtcZCzOFPtZNHXq1CErK4tt27ZFuykmRGXLls01Q8ckmIULdVpht24apBcvhjvvzP892dkwdChcfLEu8//3v+HVV+H996FVK/j8cyhVqvDt27RJ57ZbgI++UqVKUb9+/Wg3wxgTqgULtBpjmTLQpg18+qn2moPNC583T1eOnn++Pq5fH0aPhocfhn/9SyswzpgBx5XtCEuwImNxxutNt08SkXdEJENE1ojIBV6ezxgTZbt2ab0o/8K7tm21JO/69cHf9+mnWm63dOncx8uW1WX/yclalTES/Nv0WQ6+0J4H5jjnGgHNgDUen88YE03+/Hu7dvq4TRu9D5aH/+03WLVKi3vlJSkJBg+G+fP/DM6FkZGhm2WfckrhP6uY8yzAi0gV4GLgVQDn3CHnnJUdNCaeLVyoqZnWrfVx48Y6FTFYgJ8/X+87dAj8mhtv1N79Sy8Vvo3+AdZiUErAa1724OsD24AJIrJMRMaJSIXjXyQiA0UkXUTSbSDVmBi3cKEGd/8sqpIl9fEXXwR+z7x5uglGamrg19SsCVddpTsrFbbu0Zo1CZF/B28DfEmgOfCScy4V+B245/gXOefGOufSnHNpNXPubG6MiS27d+ucd396xq9NG50Xn0fhPEADfLt2mooJ5tZbtRTvW2+F38Y9e3QWTQLk38HbAJ8FZDnnvvY9fgcN+MaYePTFFzrV8fjKpm3a6PFvvjnxPevWQWZm4Px7Tuefr4unXngh731PQ+HfSc168IXjnPsN+FVE/MUeOgKBd8QwxsS2hQt1nrp/qqPf+edrvjuvPPy8eXofSoAX0V78ihXw5ZfhtTGBpkiC97NobgcmicgKIAX4X4/PZ4yJlgULNN9evnzu4yedBOedl3eA//RT3Qov1IB77bU6AybcKZNr1ui4wJlnhvf+GONpgHfOLffl15s657o75/7r5fmMMVGydy8sWXJiesavTRvtdecsOeCcBvgOHUKf0VKhgm6fN20abN1a8HZmZMBZZ0VmRWwMKPa1aIwxMeCLL3QDjWABfvfu3PPYV6+GLVuCT4/My5AhWqFy/PiCvW/fPh0HSJD0DFiAN8ZEwsKFmvrwL2w6Xl4LngqSf8+pUSP9Unj5Zf1SCYVzMGgQbN6s9W4ShAV4Y0zhLVwILVtqCiUvZ50FNWrkng//6adwxhlw+ukFP9+tt2r5g3z2Qj7mpZd0euU//6n7rCYIC/DGmML5/Xfd2CPYxu8i2ov39+CPHNFB2YL23v26doXTTgttsPWbb2DYMOjc+c8SxgnCArwxpnAWL9aAffwCp+O1aQM//QTbtsGyZZqTL2j+3a9UKRg4EObMgYkTA8+L37EDeveG2rXh9dehRGKFvMS6WmNM5C1cqKtQA+Xf/dq21fsvv/wz/16YdMntt+s5BwyAyy6Dn3/O/Xx2NvTrp8XMpk2DatXCP1eMsgBvjCmcBQt0F6ZKlYK/rkUL7XkvXqz598aN4eSTwz9vtWqwaJHm17/5Bpo0gSef/HNP18ce0x7+mDGQlhb+eWKYBXhjTPj279fgml96BnRDj+bNtXrk55+Hn3/PqUQJLSW8ejV06gR3362Dvc8/r5uFXHedpnISVLHf0ckYUwzMmQNvvAEHDuhG2gcP6v3u3dpjDjbAmlObNvDss/pzuPn3vNSuDe+9p7f/+R8dVG3cWKdSJkBZ4ECsB2+MCcw5ePppuOIKzZv/9JOuIP3jD637XreuriwNNZfuz9OXKBH6l0JB9Oypi6meeAJmzjyxbEKCsR68MSZvhw7pfPNXX9W9UF97rfAB0x/g09KgSpXCtzEvVarAyJHefHaMsQBvjDnRzp3Qq5cOoN53ny4QisQUw9NO0/noXboU/rNMvizAG2Ny+/FHDcDr1+vc8euui+znf/hhZD/PBGQB3hjzp4ULoXt3nc746ad/zl03MckCvDFG7d6tqz5POQVmz4b69aPdIlNIFuCNMerxx2H7dp0SacE9Ltg0SWOM7o367LNw/fW6GMnEBQvwxhitspiUBKNGRbslJoI8TdGISCawFzgKHHHOJWZBCGOKsy+/hClT4KGHdEWoiRtFkYNv75zbXgTnMcYUlHMwfLhufD1iRLRbYyLMBlmNSWRTpsDXX8OECYF3YzIxy+scvAP+IyJLRCRxS7oZUxwdOKDVF1NTdXDVxB2ve/AXOuc2ikgt4GMRyXDOLcr5Al/gHwhQr149j5tjjDnm+edhwwbdESnBdjpKFJ7+X3XObfTdbwXeB1rl8Zqxzrk051xazZo1vWyOMfEpI0OLgk2dCnv2hPaeLVvgf/8XunVLqE2oE41nAV5EKohIJf/PwGXASq/OZ0xM2LABXnlFC3mdfDJMmlS4zztwQD/rpZfg6quhRg3d+OLFFyErK/dr9+3Tcr8LF8LQofreJ58s3PlNseZliuZk4H3RYvslgbecc3M8PJ8xxcu+fbBxI6xdC598AnPnaq1y0DrqSUm6evTaa8PflGLECN3NaPZsqFgRZszQ22236a1xY92QY9Mm2Ls393tHjoQGDQp3jaZYExdoN/IoSEtLc+np6dFuhjEF98MPMHo0/PqrBvWNG3MH1DJldIOLTp3gr3+Fc87R3PeNN2qP+uKLC37OmTOha1e4807dlMPPOU3bzJih5X4rV9ZpkKed9uetdm1o2DChdzuKFyKyJNAaIwvwxkRC//7w9tu6sbQ/gNaurT/Xrav7hB6/Wcb+/VCnDlx2GUyeXLDzbd4MTZvq+7/6Sr9ATEIKFuBtHrwxhfXHHzB9OlxzjfbKQ1W+PAwYAGPGwG+/aRXHUGRn6xfK77/DW29ZcDcB2dwoYwpr7lwttdunT8HfO3gwHDkC48aF/p7nnoP//EeLg51zTsHPaRKGBXhjCmvKFKheHTp2LPh7zz5bUzT//rcG+vwsX66Fwbp1g4G2dtAEZwHemMLYvx8++AB69tRdkMJx6606pTG/rez279c0UPXq2uO3AVKTDwvwxhTG7Nk6HfLqq8P/jM6ddSD2xRcDv8Y5Tef88AO88YbOdzcmHxbgjSmMKVOgVi2dAhmukiVh0CD4+GPd8DovDzyggf2RR8JLBZmEZAHemHDt2wezZuk+piULOSHtpps0xfPyyyc+9/LLuhHHLbfA/fcX7jwmoViANyZcM2fqcv/CpGf8TjlFSw5MmKC5dj//qtTOnTWFY3l3UwAxH+AnTYLkZC2Gl5xc+NIexoRsyhRdzNS2bWQ+79ZbYdeuPxc9ffmlDqq2aKHnKuxfCSbhxHSAnzRJZ4qtX69jUOvX62ML8sZzu3bBRx9peiZSpXYvvFBrx7zwgubi//Y3XQn74Ye2GYcJS0wH+Pvuy/3XLOjj++6LTntMApkxAw4dikx6xk9Ee/FLl8JFF+kXx5w5OohrTBhiOsBv2FCw48ZEzJQpcPrp0Lp1ZD+3Xz+tCukfwD3rrMh+vkkoMR3gA20AZRtDGU/t2KFTGq++OvKDnpUq6cKphQu1QJkxhRDTAX7UqBML9JUvr8eN8cz772tZgUimZ3Jq3x7S8iwOaEyBxHSA79sXxo7Vv5RF9H7sWD1ujGcmT9bUSWpqtFtiTFAxP++qb18L6KYIbdkC8+drwS+bk26KuZjuwRtT5N59V+uxh1Ma2JgiZgHemFAtWAD/+Ac0a6bz1Y0p5jwP8CKSJCLLRCSfWqjGFNKHH8Jf/qK7HB0+HNnPnjJF91I97TSdA29MDCiKHvwdwJoiOI9JdE8+CZ98ooMyycnw+OOwc2fhP/eZZzQl07o1fP65juYbEwM8DfAiUgfoDBRgPzJjwrBpkwbfhx/WBULnnqvplDp1YMgQraNeUNnZMHw4/P3vcOWVuk1etWoRb7oxXvG6B/8cMBLIDvQCERkoIukikr5t2zaPm2Pi1rvvakGiq66CK67QhUgrVsC112qFxnPPhVdfDf3zDh7UXvtzz8Edd2iKpmxZ79pvjAc8C/Ai0gXY6pxbEux1zrmxzrk051xazZo1vWqOiXdTp0KTJrk3oW7SRLe227BB9z29+Wb4v//L/7M2bYJLL4Vp0+DppzXIR6qgmDFFyMvf2rZAVxHJBCYDHUTkTQ/PZxLVxo3wxRda2TEvtWrB9OnQowcMHQpPPBH4sz7+GFJSdHPrKVPgzju9abMxRSCkAC8iFUSkhO/nBiLSVUSC7jDsnLvXOVfHOZcM9AE+dc71K3SLjTmePz0TKMADlCmjAfvaa+Gee+DBB/U9fkePav7+r3/VL4T0dE33GBPDQl3Jugi4SESqAv8BvgWuBmwNqYm+qVOhaVNo1Cj460qVgtdf14JFjz4Kv/8OTz0FW7fqzJt58+CGG7Qeu9VfN3Eg1AAvzrn9InIT8KJz7kkRWR7qSZxzC4AFYbTPmOD86ZlHHw3t9UlJ8O9/Q7lyOv0xKws++wz++18YPx4GDPC2vcYUoZADvIhcgPbYb/IdS/KmScYUwDvv6H2w9MzxSpSA55/XXvro0dCwIcydq4OyxsSRUAP8MOBe4H3n3CoROQOY712zjAnR1KlaOqBhw4K9T0QXQnXurIOqFSt60z5joiikAO+cWwgsBPANtm53zg31smHG5CsrCxYvhsceC/8zLrwwcu0xppgJdRbNWyJSWUQqACuB1SIywtumGZOPcNIzxiSQUOfBn+uc2wN0Bz4C6gPXedYqY0IxdaqmVxo0iHZLjCmWQg3wpXzz3rsDHzjnDgMun/cY451ff4Uvv7TeuzFBhBrg/w1kAhWARSJyOrDHq0YZky9LzxiTr5ACvHNujHOutnPuCqfWA+09bptJdHv2wKFDeT83daruiXr22UXbJmNiSKiDrFVE5Bl/1UcReRrtzRvjjaVLte56jRraS3/tNfBXG92wAb76ykoJGJOPUOfBj0dnz/j/RV0HTAB6etEok+C+/153ZqpcWWvDzJqlKRkROP98qFpVX2fpGWOCCjXAn+mc65Xj8SMFKVVgTMjWrIGOHbWUwKefwplnalGwZct0S76ZM2H2bN1d6cwzo91aY4q1UAP8ARG50Dn3OYCItAUOeNcsk5DWrtXgXqKEFv7yB3ARaN5cbw8+CL/9ZptvGBOCUAP8YOB1Eanie/xf4AZvmmQSUmYmdOigm2UvWBC89MAppxRVq4yJaaGWKvgOaCYilX2P94jIMGCFl40zCSIrS4P7vn0wfz6cd160W2RMXCjQjk7OuT2+Fa0AttWNKbz//lfTMjt2aEXHZs2i3SJj4kaoKZq8SMRaYWJPZqZuoFG7dvif4RwMHAjr1mnPvWXLiDXPGFO4PVmtVEGiys7WXnezZrB6dfifM368Tn987DFo2zZy7TPGAPn04EVkL3kHcgHKedIiU/wtXAi//AKlS+t89c8/h/r1C/YZP/ygG2B36AB33eVNO41JcEF78M65Ss65ynncKjnnCpPeMbFswgRdhPT553DggAb5zZtDf/+hQ7oHatmyukdqicL8IWmMCcSzf1kiUlZEvhGR70RklYg84tW5TBHas0fTKn36aM78o490Xvpll8HOnaF9xgMPwJIl8OqrhcvhG2OC8rLr9AfQwTnXDEgBOonI+R6ezxSFqVO11+7fnLp1a5gxA378Ea64Qqc6BjNvHjz5JAwaBN27e99eYxKYZwHeV3XS/6+9lO9mA7OxbuJEaNRIA7tfx44wZQqkp2vQPngw7/du3w7XX6/vf+aZImmuMYnM0zy6iCQBS4CzgBecc1/n8ZqBwECAevXqedkcU1g//ghffAFPPKHlA3Lq3l1nxdxwA1x8sX4BnHaapmBq19af771XK0J++CGULx+dazAmgXga4J1zR4EUETkJeF9EGjvnVh73mrHAWIC0tDTr4RdnEyfqgOh1AXZrvP56LTXw7LPw5puwa9eJr3n6aa3jbozxXJHMhHHO7RKR+UAntOywiTVHj+qMl06d4NRTA7/uppv0BrB/P2zaBBs36n3p0tCjR9G01xjjXYAXkZrAYV9wLwf8BXjCq/MZj338sQbq554L/T3ly8NZZ+nNGFPkvOzBnwq85svDlwCmOuc+9PB8xksTJkC1avC3v0W7JcaYEHkW4J1zKwBLtsaDnTth+nSd2limTLRbY4wJkS0hNPmbPFlXn/rnvhtjYoIFeJO/CRO0sD40CfsAABA4SURBVJjNfjEmpliAN8GtXKkLmPr3j3ZLjDEFZAHeBDdhApQsqcXBjDExxQK8CWzjRp37/re/Qc2a0W6NMaaALMCbvG3bBpdeCn/8AQ89FO3WGGPCYDXdzYl279YVq5mZtk+qMTHMArzJbf9+6NIFVqzQMsAXXxztFhljwmQB3vzp0CHo1UsrRk6erPXdjTExywK8UUePQr9+MGcOvPIKXHVVtFtkjCkkG2Q1kJ0NAwfCtGlazvfmm6PdImNMBFgPPtH9/rvWcX/vPXjwQbjzzmi3yBgTIRbgE1lWFnTtCt99p1voDRsW7RYZYyLIAnyi+uYb6NZNe/AzZ9qAqjFxyHLwiWjyZLjkEihXDr780oK7MXHKAnwicE576r/9pqtSr7kG0tLg66/hvPOi3TpjjEcsRRNP9uyBWbN0wHTFCti7F/bt05vLsZ/5gAHw0ku2eYcxcc4CfKzbtg0++ECD+ief6GKlU06Biy6CKlWgYkWoVOnP+9NPh8svB5Fot9wY4zEL8LFq+XK4/3746COdx56cDLffDj17wvnnQwnLvhmT6DwL8CJSF3gdOBlwwFjn3PNenS9hrF+vgX3SJKhaFe69F668UguCWa/cGJODlz34I8DfnXNLRaQSsEREPnbOrfbwnPFr504YNQr+9S/tnY8cCffcAyedFO2WGWOKKc8CvHNuM7DZ9/NeEVkD1AYswBdEdjY8+yw89piW8e3fHx55BOrWjXbLjDHFXJEkakUkGUgFvs7juYEiki4i6du2bSv4h+/dq7NC3n23sM0sfg4ehD594K674IILdMXp+PEW3I0xIfE8wItIReBdYJhzbs/xzzvnxjrn0pxzaTXD2RauXDlYsgRGjNCAGC927NAdlaZNgyef1OmPTZpEu1XGmBjiaYAXkVJocJ/knHvPk5OULKkpjHXr4Pk4GcP9+Wdo0wa+/VZXnY4YYQOoxpgC8yzAi4gArwJrnHPPeHUeADp21I2hR42CLVsAnWSSnKzjkcnJ+vgY5+DAAU+bFLavv9Z0zLZtOq/96quj3SJjTIzysgffFrgO6CAiy30374qePPWUBu0HHmDSJC1vvn69xvL16/XxpEnogX794OSTYfp0z5oTlhkzoH17XZT05Ze6WMkYY8LkWYB3zn3unBPnXFPnXIrvNtur89GggS70GTeOSSOWs39/7qf374f77kNnoLz1FlSuDD16wMMP60yVaPvgA21Pkybw1VfQsGG0W2SMiXHxtdzxgQegWjVGbB6Orq3Kre36tzTADxgAP/0EN9ygj3v21Dou0bJ2rW660bw5zJ8PtWpFry3GmLgRXwG+alV45BHas4BuzMj11AUsZjwD4OKL4eWXdfbNhAk6MPvhh7q8/8cfi77N+/frRtclSsA770D58kXfBmNMXIqvAA8waBC7ap/L03IXpfkDgNPJZAbdOVSrrhblKl1aXysCQ4fCxx/D1q3QqhXM9i6LdALn4NZb4fvv/xwVNsaYCIm/AF+yJCe9+gxnup95oOq/qMwe5pbqQuXyh6m08EOoXv3E97RvD+npGmC7dNEefVEYNw5ee01TS5dfXjTnNMYkDHHuxFx1tKSlpbn09PTIfNgVV8AXX0DLlrBgAcydq9Mpg/n9d03h/PSTbmnXqFFk2pKX9HRo2xbatdO/GpKSvDuXMSZuicgS51xaXs/FXw/e7+mnNWDPmwcvvph/cAeoUAHefx/KltX9SnftCv/8hw/Dpk25N9rw27lTK0CefLKmZiy4G2M8EL/14M85RysvHjyok+BDVa+e1rXp0AH69tXpiwUNwFu26PtXr9aUUEoKpKbqfUqKrkzdtAk+/xxq1CjYZxtjTIjitwcPMHgwDBuW61DQFa5+F10EY8Zo6uSBBwp2zq1bNbhnZurK2h499C+BMWN0gVXjxrpJx/PP66CuMcZ4JH578Hnwr3D1L4Lyr3AF7aznMngwLFsGjz+uve6rrsr/BNu2/RncZ8+GSy7587nDhyEjQ3diOnpU5+AbY4yH4neQNQ/JyRrUj3f66RqTT3DokM6wWb4cFi/WXZMC2bZN8/xr12rlx/btI9RqY4wJLDEHWfOwYUPBjlO6tObjq1bVQdf58/MuUrZ9u5b2/eknnWJpwd0YUwwkVICvV69gxwE45RSdWbNjh6ZfTjpJc/T3368LpDZs0OD+448wc6a+xhhjioGECvCjRp1YCaB8eT0eVMuWkJWlvfM77tDUzejRcNllmt/JyNBKkJde6lnbjTGmoBJqkNU/kHrffdrxrldPg/sJA6x5qVIFOnfWG+hWgYsXa+XHv/xFN+gwxphiJKEGWYOZNCnMwG+MMVEUbJA1oXrwgRRo+qQxxsSIhMrBB3LffQTeIMQYY2KUBXjCmD5pjDExwMtNt8eLyFYRWenVOSIlrOmTxhhTzHnZg58IdPLw8yMmv+mTIdWvMcaYYsbLTbcXATu9+vxI6tsXxo7VKe0iej92rB73D8CuX6+Vf/0DsBbkjTHFnafTJEUkGfjQOdc4lNdHc5pkIAWuX2OMMUWoWNeiEZGBIpIuIunbtm2LdnNOYAOwxphYFfUA75wb65xLc86l1axZM9rNOYENwBpjYlXUA3xxF2wA1gZfjTHFmZfTJN8GvgQaikiWiNzk1bm8FGgAFmzw1RhTvFktmjDZ4Ksxpjgo1oOsscoGX40xxZ0F+DDZ4KsxprizAB8mG3w1xhR3FuDDZIOvxpjizgZZI8wGX40xRckGWYtQsMFXS90YY4qSBfgICzTIWq2apW6MMUXLAnyEBRp8Bds1yhhTtCzAR1igwdedAQonW+rGGOMVG2QtIoEGX6tXhwMHcvfuy5f/sx69McYEY4OsxYClbowxRc0CfBEJJ3UDlr4xxoTPUjRRFmze/KhROtPG0jfGmEAsRVOMBSt5cN99gdM31rM3xuTHAnyUBdvwO9CiKf8c+rzm1FvgN8b4WYqmGAuUvklKgqNHTzwebEYOaM9/wwZdjDVqlKV5jIkHlqKJUYHSN3kFd4AdO/JO6dxxR/BVtIF6/ZE6boyJEudcsbm1aNHCmdzefNO50093TkTv/Y81VBfu5v+88uVzHy9f3rkhQyJz/M03876GYNdXFMdj7RwF+f0wiQVIdwFiatSDes6bBfjQBArK1asXLMD7g0JezyUlReZ49erBA7+XXy75fenEyjkCfUkG+vyi+pK0L25vzxGqqAV4oBPwA7AWuCe/11uAD11B/sEHCvz+90fir4Fw/nrw+ssl0PGiOHckzxHoSzLY/9d4/vKMly/uYOcoiKgEeCAJ+Bk4AygNfAecG+w9FuALr6A9Pa8DXaCbSPS+XIri3NG+vnj+8oyXL+5g5yiIaAX4C4C5OR7fC9wb7D0W4L0T7E9EL3sjwXqZFghCO17QWzT/Mov2F1s8fHGLFOzfdrQC/JXAuByPrwP+lcfrBgLpQHq9evUKdmUmIrzMJwb768H+lA/teKAvyWDjG/H85RkvX9yx3oMPKcDnvFkPPj7ZYJy3X5LR+MvMcvDenqMgLEVjTIwLZ7ZFPH95xsMXd37nCFWwAO/ZSlYRKQn8CHQENgLfAtc651YFeo+tZDXGmIIJtpK1pFcndc4dEZH/AeaiM2rGBwvuxhhjIsuzAA/gnJsNzPbyHMYYY/JmtWiMMSZOWYA3xpg4ZQHeGGPiVLGqBy8i24A8KqCHpAawPYLNiRV23YnFrjuxhHLdpzvnaub1RLEK8IUhIumBpgrFM7vuxGLXnVgKe92WojHGmDhlAd4YY+JUPAX4sdFuQJTYdScWu+7EUqjrjpscvDHGmNziqQdvjDEmBwvwxhgTp2I+wItIJxH5QUTWisg90W6Pl0RkvIhsFZGVOY5VE5GPReQn333VaLYx0kSkrojMF5HVIrJKRO7wHY/r6wYQkbIi8o2IfOe79kd8x+uLyNe+3/kpIlI62m2NNBFJEpFlIvKh73HcXzOAiGSKyPcislxE0n3Hwv5dj+kALyJJwAvA5cC5wDUicm50W+WpiehG5jndA8xzzp0NzPM9jidHgL87584Fzgdu8/0/jvfrBvgD6OCcawakAJ1E5HzgCeBZ59xZwH+Bm6LYRq/cAazJ8TgRrtmvvXMuJcf897B/12M6wAOtgLXOuV+cc4eAyUC3KLfJM865RcDO4w53A17z/fwa0L1IG+Ux59xm59xS38970X/0tYnz6wbw7eewz/ewlO/mgA7AO77jcXftIlIH6AyM8z0W4vya8xH273qsB/jawK85Hmf5jiWSk51zm30//wacHM3GeElEkoFU4GsS5Lp9qYrlwFbgY+BnYJdz7ojvJfH4O/8cMBLI9j2uTvxfs58D/iMiS0RkoO9Y2L/rntaDN0XLOedEJC7nvYpIReBdYJhzbo926lQ8X7dz7iiQIiInAe8DjaLcJE+JSBdgq3NuiYi0i3Z7ouBC59xGEakFfCwiGTmfLOjveqz34DcCdXM8ruM7lki2iMipAL77rVFuT8SJSCk0uE9yzr3nOxz3152Tc24XMB/d6/gk35aYEH+/822BriKSiaZcOwDPE9/XfIxzbqPvfiv6hd6KQvyux3qA/xY42zfCXhroA3wQ5TYVtQ+AG3w/3wDMiGJbIs6Xf30VWOOceybHU3F93QAiUtPXc0dEygF/Qccg5gNX+l4WV9funLvXOVfHOZeM/nv+1DnXlzi+Zj8RqSAilfw/A5cBKynE73rMr2QVkSvQnJ1/39dRUW6SZ0TkbaAdWkJ0C/AQMB2YCtRDSy1f5Zw7fiA2ZonIhcBnwPf8mZP9B5qHj9vrBhCRpuigWhLaGZvqnPuniJyB9m6rAcuAfs65P6LXUm/4UjR3Oee6JMI1+67xfd/DksBbzrlRIlKdMH/XYz7AG2OMyVusp2iMMcYEYAHeGGPilAV4Y4yJUxbgjTEmTlmAN8aYOGUB3sQ9ETnqq87nv0WsMJmIJOes7mlMcWKlCkwiOOCcS4l2I4wpataDNwnLV3v7SV/97W9E5Czf8WQR+VREVojIPBGp5zt+soi876vP/p2ItPF9VJKIvOKr2f4f36pTRGSor479ChGZHKXLNAnMArxJBOWOS9FcneO53c65JsC/0BXRAP8HvOacawpMAsb4jo8BFvrqszcHVvmOnw284Jw7D9gF9PIdvwdI9X3OYK8uzphAbCWriXsiss85VzGP45nohhq/+Aqa/eacqy4i24FTnXOHfcc3O+dqiMg2oE7OJfK+EsYf+zZjQETuBko55x4TkTnAPrScxPQctd2NKRLWgzeJzgX4uSBy1kQ5yp9jW53RHceaA9/mqIZoTJGwAG8S3dU57r/0/bwYrWQI0Bctdga6XdoQOLYRR5VAHyoiJYC6zrn5wN1AFeCEvyKM8ZL1KEwiKOfbFclvjnPOP1WyqoisQHvh1/iO3Q5MEJERwDZggO/4HcBYEbkJ7akPATaTtyTgTd+XgABjfDXdjSkyloM3CcuXg09zzm2PdluM8YKlaIwxJk5ZD94YY+KU9eCNMSZOWYA3xpg4ZQHeGGPilAV4Y4yJUxbgjTEmTv1/uIncBc5IIuYAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "epochs = range(50)\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-y0GdP5dAZ"
      },
      "source": [
        "## 4. Train (again) and evaluate the model (5 points)\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BItDP3uj5dAa"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3ER_tFjJ5dAc"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "model.compile(tf.keras.optimizers.RMSprop(learning_rate=lr),loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gzVRi3cL5dAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71352885-8135-4b02-9d0d-20133d1ceb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 3s 12ms/step - loss: 0.8352 - accuracy: 0.8651\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 3s 13ms/step - loss: 0.3819 - accuracy: 0.9141\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 2s 12ms/step - loss: 0.2281 - accuracy: 0.9407\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1543 - accuracy: 0.9589\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1124 - accuracy: 0.9681\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1049 - accuracy: 0.9721\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0858 - accuracy: 0.9761\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0829 - accuracy: 0.9781\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0815 - accuracy: 0.9783\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0720 - accuracy: 0.9813\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0733 - accuracy: 0.9806\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0811 - accuracy: 0.9808\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0742 - accuracy: 0.9821\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0801 - accuracy: 0.9809\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0725 - accuracy: 0.9829\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0738 - accuracy: 0.9823\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0745 - accuracy: 0.9834\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0645 - accuracy: 0.9850\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0764 - accuracy: 0.9834\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0706 - accuracy: 0.9841\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0621 - accuracy: 0.9852\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.0639 - accuracy: 0.9863\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0724 - accuracy: 0.9844\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0666 - accuracy: 0.9864\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0646 - accuracy: 0.9858\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0636 - accuracy: 0.9856\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0688 - accuracy: 0.9854\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0613 - accuracy: 0.9868\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0656 - accuracy: 0.9870\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0610 - accuracy: 0.9866\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0703 - accuracy: 0.9862\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0647 - accuracy: 0.9877\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0665 - accuracy: 0.9873\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0639 - accuracy: 0.9871\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0680 - accuracy: 0.9873\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0645 - accuracy: 0.9877\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0647 - accuracy: 0.9880\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0642 - accuracy: 0.9881\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0679 - accuracy: 0.9877\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0684 - accuracy: 0.9877\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0637 - accuracy: 0.9884\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0663 - accuracy: 0.9888\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0698 - accuracy: 0.9885\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0643 - accuracy: 0.9884\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0569 - accuracy: 0.9891\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0634 - accuracy: 0.9892\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0645 - accuracy: 0.9890\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0688 - accuracy: 0.9886\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0601 - accuracy: 0.9899\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.0708 - accuracy: 0.9884\n"
          ]
        }
      ],
      "source": [
        "#<Train your model on the entire training set (50K samples)>\n",
        "history = model.fit(np.array(x_train), np.array(y_train_vec), batch_size=bsize, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4bc4_6h5dAc"
      },
      "source": [
        "## 5. Evaluate the model on the test set (5 points)\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UV0sM8dO5dAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b25487-c413-49b7-cb22-ac63059bc5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 23s 2ms/step - loss: 10.1678 - accuracy: 0.6497\n",
            "Test accuracy = 0.6496999859809875\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "loss_and_acc = model.evaluate(np.array(x_test), np.array(y_test_vec), batch_size=1)\n",
        "\n",
        "print('Test accuracy = ' + str(loss_and_acc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObH5b0t45dAd"
      },
      "source": [
        "## 6. Building model with new structure (25 points)\n",
        "- In this section, you can build your model with adding new layers (e.g, BN layer or dropout layer, ...).\n",
        "- If you want to regularize a ```Conv/Dense layer```, you should place a ```Dropout layer``` before the ```Conv/Dense layer```.\n",
        "- You can try to compare their loss curve and testing accuracy and analyze your findings.\n",
        "- You need to try at lease two different model structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VNKWcz1O5dAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00e9ae52-87b8-4ff4-ec1a-f0757bbd393a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 626,378\n",
            "Trainable params: 626,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 3s 10ms/step - loss: 3.8138 - accuracy: 0.1319\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 2.2174 - accuracy: 0.2078\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 2.0675 - accuracy: 0.2810\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.8518 - accuracy: 0.3415\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.6290 - accuracy: 0.4203\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.4687 - accuracy: 0.4854\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.3552 - accuracy: 0.5307\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.2583 - accuracy: 0.5650\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.1977 - accuracy: 0.5873\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.1277 - accuracy: 0.6105\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.0848 - accuracy: 0.6281\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.0374 - accuracy: 0.6433\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.9991 - accuracy: 0.6565\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.9568 - accuracy: 0.6707\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.9253 - accuracy: 0.6824\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.8928 - accuracy: 0.6962\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.8583 - accuracy: 0.7047\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.8362 - accuracy: 0.7162\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.8148 - accuracy: 0.7208\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.8029 - accuracy: 0.7301\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.7721 - accuracy: 0.7377\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.7551 - accuracy: 0.7423\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.7381 - accuracy: 0.7499\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.7151 - accuracy: 0.7579\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6991 - accuracy: 0.7633\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6860 - accuracy: 0.7677\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6588 - accuracy: 0.7775\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6606 - accuracy: 0.7770\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6421 - accuracy: 0.7825\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6248 - accuracy: 0.7884\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6087 - accuracy: 0.7914\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.6061 - accuracy: 0.7957\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5837 - accuracy: 0.8036\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5837 - accuracy: 0.8047\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5766 - accuracy: 0.8062\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5673 - accuracy: 0.8100\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5620 - accuracy: 0.8108\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5527 - accuracy: 0.8137\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5404 - accuracy: 0.8179\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5338 - accuracy: 0.8234\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5312 - accuracy: 0.8223\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5208 - accuracy: 0.8259\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5168 - accuracy: 0.8274\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.5073 - accuracy: 0.8293\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.5039 - accuracy: 0.8310\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.4923 - accuracy: 0.8348\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.4927 - accuracy: 0.8361\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.4816 - accuracy: 0.8399\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.4801 - accuracy: 0.8415\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.4751 - accuracy: 0.8415\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 22s 2ms/step - loss: 1.0348 - accuracy: 0.6765\n",
            "Test accuracy = 0.6765000224113464\n"
          ]
        }
      ],
      "source": [
        "#New model version 1: add a dropout layer\n",
        "import matplotlib.pyplot as plt\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape = (32, 32, 3))) #3-by-3 convolution to get from 32x32 to 30x30\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (4,4), activation='relu', input_shape=(15,15,32)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(tf.keras.optimizers.RMSprop(learning_rate=0.0008),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(np.array(x_train), np.array(y_train_vec), batch_size=256, epochs=50)\n",
        "epochs = range(50)\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "loss_and_acc = model.evaluate(np.array(x_test), np.array(y_test_vec), batch_size=1)\n",
        "\n",
        "print('Test accuracy = ' + str(loss_and_acc[1]))\n",
        "\n",
        "\n",
        "#As we can see, adding one dropout layer increases the testing accuracy to around 70%\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New model version 2: add a batch normalization layer at the start\n",
        "import matplotlib.pyplot as plt\n",
        "model2 = models.Sequential()\n",
        "model2.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3))) #3-by-3 convolution to get from 32x32 to 30x30\n",
        "model2.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model2.add(layers.MaxPooling2D(2,2))\n",
        "model2.add(layers.Conv2D(64, (4,4), activation='relu', input_shape=(15,15,32)))\n",
        "model2.add(layers.MaxPooling2D(2,2))\n",
        "model2.add(layers.Flatten())\n",
        "model2.add(layers.Dropout(0.5))\n",
        "model2.add(layers.Dense(256, activation='relu'))\n",
        "model2.add(layers.Dense(10, activation='softmax'))\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(tf.keras.optimizers.RMSprop(learning_rate=0.0008),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model2.fit(np.array(x_train), np.array(y_train_vec), batch_size=256, epochs=50)\n",
        "epochs = range(50)\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "loss_and_acc = model2.evaluate(np.array(x_test), np.array(y_test_vec), batch_size=1)\n",
        "\n",
        "print('Test accuracy = ' + str(loss_and_acc[1]))\n",
        "\n",
        "\n",
        "#Adding the BN layer gives us another increase in testing accuracy"
      ],
      "metadata": {
        "id": "LcrtC9alAZ7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30a2169e-80c4-450c-b073-f6f6fd66909c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 30, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 626,506\n",
            "Trainable params: 626,442\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 3s 12ms/step - loss: 1.6345 - accuracy: 0.4290\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 1.2463 - accuracy: 0.5665\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 1.0922 - accuracy: 0.6225\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.9878 - accuracy: 0.6590\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.9134 - accuracy: 0.6839\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.8443 - accuracy: 0.7079\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.7856 - accuracy: 0.7273\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.7312 - accuracy: 0.7483\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.6907 - accuracy: 0.7623\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.6538 - accuracy: 0.7735\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.6158 - accuracy: 0.7872\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.5859 - accuracy: 0.7983\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.5587 - accuracy: 0.8066\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.5297 - accuracy: 0.8163\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.5169 - accuracy: 0.8221\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.4892 - accuracy: 0.8303\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.4644 - accuracy: 0.8396\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.4490 - accuracy: 0.8451\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.4348 - accuracy: 0.8490\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.4172 - accuracy: 0.8544\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.4010 - accuracy: 0.8593\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3871 - accuracy: 0.8645\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3789 - accuracy: 0.8678\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3671 - accuracy: 0.8724\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3574 - accuracy: 0.8751\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3485 - accuracy: 0.8795\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3416 - accuracy: 0.8821\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3312 - accuracy: 0.8856\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3198 - accuracy: 0.8887\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3133 - accuracy: 0.8908\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.3065 - accuracy: 0.8937\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2977 - accuracy: 0.8982\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2933 - accuracy: 0.8990\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2862 - accuracy: 0.9005\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2809 - accuracy: 0.9022\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2772 - accuracy: 0.9043\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2673 - accuracy: 0.9079\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2746 - accuracy: 0.9052\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2655 - accuracy: 0.9084\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2595 - accuracy: 0.9111\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2523 - accuracy: 0.9119\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2532 - accuracy: 0.9126\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2461 - accuracy: 0.9155\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2435 - accuracy: 0.9166\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2387 - accuracy: 0.9176\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2385 - accuracy: 0.9188\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2330 - accuracy: 0.9206\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2311 - accuracy: 0.9220\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2314 - accuracy: 0.9193\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2268 - accuracy: 0.9240\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcDElEQVR4nO3de5RU5Znv8e+PBkW8T9PmQgsNM3gbNBhbjJq1hJxciLg056gZmXZGEx3EY8Q4OfESVqKTkaWeWSeZeKJxMONoRgY1x8RDEqImjpGsmEQaFQNechwEbOPIRUUddRR4zh97FzRNVXVXd++67d9nrVpd+927qp4tbT/17me/76uIwMzM8mtErQMwM7PaciIwM8s5JwIzs5xzIjAzyzknAjOznBtZ6wAqNXbs2Ojo6Kh1GGZmDWXFihWbIqKt2L6GSwQdHR10d3fXOgwzs4YiaV2pfb40ZGaWc04EZmY550RgZpZzDVcjMLP68t5779HT08M777xT61AMGD16NO3t7YwaNWrAr3EiMLMh6enpYd9996WjowNJtQ4n1yKCzZs309PTw8SJEwf8ulxcGlq0CDo6YMSI5OeiRbWOyKx5vPPOO7S2tjoJ1AFJtLa2Vtw7a/oewaJFMGcOvPVWsr1uXbIN0NVVu7jMmomTQP0YzL9F0/cI5s/fmQQK3noraTczsxwkgvXrK2s3s8ayefNmpk6dytSpU3n/+9/PuHHjdmy/++67ZV/b3d3NvHnz+v2ME044YVhi/cUvfsEpp5wyLO81nJo+EYwfX1m7mWVruGt2ra2tPPHEEzzxxBPMnTuXSy+9dMf2HnvswdatW0u+trOzkxtuuKHfz3jkkUeGFmSdyywRSLpV0gZJq8ocM13SE5JWS3o4izgWLIAxY3ZtGzMmaTez6irU7Natg4idNbvhvoHj3HPPZe7cuRx33HFcdtllPProoxx//PEcffTRnHDCCTz77LPArt/Qr776aj7/+c8zffp0Jk2atEuC2GeffXYcP336dM444wwOO+wwurq6KKzyuHTpUg477DCOOeYY5s2bV9E3/8WLF3PkkUcyZcoULr/8cgC2bdvGueeey5QpUzjyyCP55je/CcANN9zAEUccwVFHHcVZZ5019P9YZFssvg34NvC9YjslHQDcBMyMiPWSDsoiiEJBeP785HLQ+PFJEnCh2Kz6ytXshvv/yZ6eHh555BFaWlp4/fXX+eUvf8nIkSP5+c9/zle+8hXuueee3V7zzDPP8NBDD/HGG29w6KGHcuGFF+52P/7jjz/O6tWr+eAHP8iJJ57Ir371Kzo7O7ngggtYtmwZEydOZPbs2QOO8w9/+AOXX345K1as4MADD+STn/wk9957LwcffDAvvvgiq1Yl36Vfe+01AK677jqef/559txzzx1tQ5VZjyAilgGvlDnkz4EfRMT69PgNWcXS1QVr18L27clPJwGz2qhmze7MM8+kpaUFgC1btnDmmWcyZcoULr30UlavXl30NbNmzWLPPfdk7NixHHTQQbz88su7HTNt2jTa29sZMWIEU6dOZe3atTzzzDNMmjRpx737lSSC5cuXM336dNra2hg5ciRdXV0sW7aMSZMmsWbNGi6++GLuu+8+9ttvPwCOOuoourq6uOOOOxg5cni+y9eyRnAIcKCkX0haIekvSx0oaY6kbkndGzdurGKIZjacqlmz23vvvXc8/+pXv8qMGTNYtWoVP/rRj0reZ7/nnnvueN7S0lK0vjCQY4bDgQceyMqVK5k+fTo333wz559/PgA/+clPuOiii3jsscc49thjh+Xza5kIRgLHALOATwFflXRIsQMjYmFEdEZEZ1tb0em0zawB1Kpmt2XLFsaNGwfAbbfdNuzvf+ihh7JmzRrWrl0LwF133TXg106bNo2HH36YTZs2sW3bNhYvXsxJJ53Epk2b2L59O6effjrXXHMNjz32GNu3b+eFF15gxowZXH/99WzZsoU333xzyPHXckBZD7A5Iv4D+A9Jy4APAb+vYUxmlqFa1ewuu+wyzjnnHK655hpmzZo17O+/1157cdNNNzFz5kz23ntvjj322JLHPvjgg7S3t+/Y/v73v891113HjBkziAhmzZrFaaedxsqVK/nc5z7H9u3bAbj22mvZtm0bZ599Nlu2bCEimDdvHgcccMCQ41eh4p0FSR3AjyNiSpF9h5MUkz8F7AE8CpwVESXvMgLo7OwML0xjVj+efvppDj/88FqHUXNvvvkm++yzDxHBRRddxOTJk7n00ktrEkuxfxNJKyKis9jxmfUIJC0GpgNjJfUAVwGjACLi5oh4WtJ9wJPAduC7/SUBM7N6dcstt3D77bfz7rvvcvTRR3PBBRfUOqQBy7RHkAX3CMzqi3sE9afSHkHTjyw2s+w12hfKZjaYfwsnAjMbktGjR7N582YngzpQWI9g9OjRFb2u6aehNrNstbe309PTg8f41IfCCmWVcCIwsyEZNWpURathWf3xpSEzs5xzIjAzyzknAjOznHMiMDPLOScCM7OccyIwM8s5JwIzs5xzIjAzyzknAjOznHMiMDPLOScCM7OccyIwM8s5JwIzs5xzIjAzyzknAjOznMssEUi6VdIGSWUXpJd0rKStks7IKhYzMystyx7BbcDMcgdIagGuBx7IMA4zMysjs0QQEcuAV/o57GLgHmBDVnGYmVl5NasRSBoH/FfgOwM4do6kbkndXhfVzGx41bJY/PfA5RGxvb8DI2JhRHRGRGdbW1sVQjMzy49aLl7fCdwpCWAscLKkrRFxbw1jMjPLnZolgoiYWHgu6Tbgx04CZmbVl1kikLQYmA6MldQDXAWMAoiIm7P6XDMzq0xmiSAiZldw7LlZxWFmZuV5ZLGZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc7lPhEsWgQdHTBiRPJz0aJaR2RmVl21XKGs5hYtgjlz4K23ku1165JtgK6u2sVlZlZNue4RzJ+/MwkUvPVW0m5mlhe5TgTr11fWbmbWjHKdCMaPr6zdzKwZ5ToRLFgAY8bs2jZmTNJuZpYXmSUCSbdK2iBpVYn9XZKelPQ7SY9I+lBWsZTS1QULF8KECSAlPxcudKHYzPIly7uGbgO+DXyvxP7ngZMi4lVJnwYWAsdlGE9RXV3+w29m+ZZZIoiIZZI6yux/pNfmb4D2rGIxM7PS6qVGcB7w01I7Jc2R1C2pe+PGjVUMy8ys+dU8EUiaQZIILi91TEQsjIjOiOhsa2urXnBmZjlQ05HFko4Cvgt8OiI21zIWM7O8qlmPQNJ44AfAX0TE72sVh5lZ3mXWI5C0GJgOjJXUA1wFjAKIiJuBrwGtwE2SALZGRGdW8ZiZWXFZ3jU0u5/95wPnZ/X5ZmY2MDUvFpuZWW05EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE0EJXtTezPIi14vXl+JF7c0sT9wjKMKL2ptZnjgRFOFF7c0sT5wIivCi9maWJ04ERXhRezPLEyeCIryovZnlie8aKsGL2ptZXrhHYGaWc04EZmY550RgZpZzTgRmZjnnRGBmlnOZJQJJt0raIGlVif2SdIOk5yQ9KenDWcViZmalZdkjuA2YWWb/p4HJ6WMO8J0MYzEzsxIySwQRsQx4pcwhpwHfi8RvgAMkfSCreIaLp6c2s2ZTyxrBOOCFXts9adtuJM2R1C2pe+PGjVUJrpjC9NTr1kHEzumpnQzMrJE1RLE4IhZGRGdEdLa1tdUsDk9PbWbNaECJQNLekkakzw+RdKqkUUP87BeBg3ttt6dtdcvTU5tZMxpoj2AZMFrSOOAB4C9IisFDsQT4y/TuoY8AWyLipSG+Z6Y8PbWZNaOBJgJFxFvAfwNuiogzgT8t+wJpMfBr4FBJPZLOkzRX0tz0kKXAGuA54Bbgvw/qDKrI01ObWTMa6OyjknQ80AWcl7a1lHtBRMzuZ38AFw3w8+tCYTbS+fOTy0HjxydJwLOUmlkjG2gi+CJwJfDDiFgtaRLwUHZh1S9PT21mzWZAiSAiHgYeBkiLxpsiYl6WgZmZWXUM9K6hf5G0n6S9gVXAU5K+nG1oZmZWDQMtFh8REa8DnwF+CkwkuXPIzMwa3EATwah03MBngCUR8R4Q2YXVeDz1hJk1qoEWi/8BWAusBJZJmgC8nlVQjaYw9URh1HFh6glwYdnM6p+SuzgH8UJpZERsHeZ4+tXZ2Rnd3d3V/tiyOjqSP/59TZgAa9dWOxozs91JWhERncX2DbRYvL+kbxQmfpP0v4C9hzXKBuapJ8yskQ20RnAr8Abw2fTxOvBPWQXVaDz1hJk1soEmgj+OiKsiYk36+BtgUpaBNRJPPWFmjWygieBtSR8tbEg6EXg7m5AaT1cXLFyY1ASk5OfChS4Um1ljGOhdQ3OB70naP91+FTgnm5Aak6eeMLNGNdApJlYCH5K0X7r9uqQvAk9mGZyZmWWvohXKIuL1dIQxwF9nEE/T8UAzM6t3A700VIyGLYom5YFmZtYIhrJmsaeY6IfXODazRlC2RyDpDYr/wRewVyYRNREPNDOzRlA2EUTEvtUKpBmNH1986gkPNDOzejKUS0PWDw80M7NGkGkikDRT0rOSnpN0RZH94yU9JOlxSU9KOjnLeKrNA83MrBEMevbRft9YagF+D3wC6AGWA7Mj4qlexywEHo+I70g6AlgaER3l3rceZx81M6t3Q559dJCmAc+lcxO9C9wJnNbnmAD2S5/vD/whw3jqjscYmFk9GMo4gv6MA17otd0DHNfnmKuBByRdTDKt9ceLvZGkOcAcgPFNUmn1GAMzqxe1LhbPBm6LiHbgZOCfJe0WU0QsjIjOiOhsa2urepBZ8BgDM6sXWSaCF4GDe223p229nQfcDRARvwZGA2MzjKlueIyBmdWLLBPBcmCypImS9gDOApb0OWY98F8AJB1Okgg2ZhhT3fBiNmZWLzJLBOl6xl8A7geeBu6OiNWSvi7p1PSwLwF/JWklsBg4N7K6janOeIyBmdWLLIvFRMRSYGmftq/1ev4UcGKWMdSrQkF4/vzkctD48UkScKHYzKot00Rg5XkxGzOrB7W+a8iK8PgCM6sm9wjqjMcXmFm1uUdQZzy+wMyqzYmgznh8gZlVmxNBnfH4AjOrNieCOuPxBWZWbU4EdabcGga+m8jMsuC7hupQsfEFvpvIzLLiHkGD8N1EZpYVJ4IG4buJzCwrTgQNwncTmVlWnAgaRLm7iVxENrOhcCJoEKXuJoKkaLxuHUTsLCI7GZjZQKnRpv/v7OyM7u7uWodRNzo6kj/+fU2YAGvXVjsaM6tXklZERGexfe4RNDgXkc1sqJwIGpyLyGY2VE4EDa6/KSlcSDaz/jgRNLj+pqRwIdnM+pNpsVjSTOBbQAvw3Yi4rsgxnwWuBgJYGRF/Xu49XSweOBeSzaygXLE4s7mGJLUANwKfAHqA5ZKWpAvWF46ZDFwJnBgRr0o6KKt48siFZDMbiCwvDU0DnouINRHxLnAncFqfY/4KuDEiXgWIiA0ZxpM75QrJrh2YWUGWiWAc8EKv7Z60rbdDgEMk/UrSb9JLSTZMShWSTz7ZtQMz26nWxeKRwGRgOjAbuEXSAX0PkjRHUrek7o0bN1Y5xMZVqpC8dKlnMjWznbJMBC8CB/fabk/beusBlkTEexHxPPB7ksSwi4hYGBGdEdHZ1taWWcDNqKsrKQxv35787Opy7cDMdpVlIlgOTJY0UdIewFnAkj7H3EvSG0DSWJJLRWsyjMlw7cDMdpVZIoiIrcAXgPuBp4G7I2K1pK9LOjU97H5gs6SngIeAL0fE5qxisoRrB2bWmyedy6lFi5KawPr1SU9gwYJk2+MOzJpTuXEETgS2w4gRSU+gLympMZhZ4/LsozYgrh2Y5ZMTge3g2oFZPjkR2A6DGXfgnoJZ43ONwPpVqnYASY+hd5IYM2bn7KdmVj9cI7AhKVU7aGnxCGWzZuBEYP0qVTvYtq348R6hbNZYnAisX6VqBxMmFD++0INw/cCsMbhGYINWWAGtWI0ASu9z/cCs+lwjsEyUWyZz/nzXD8wahXsElgmPUjarL+4RWNV5lLJZ43AisEx4lLJZ43AisEx4lLJZ43CNwKrKo5TNasM1Aqsbgxml7J6CWbacCKyqKh2lXKghuKZglh0nAquqSkcpu6dglj3XCKwulBql3DcJ9OaagtnA1axGIGmmpGclPSfpijLHnS4pJBUN0prfcPYUwL0Fs0qMzOqNJbUANwKfAHqA5ZKWRMRTfY7bF7gE+G1WsVhj6Ooq/m2+kp7C+vW79y4KdYXCZ5jZrrLsEUwDnouINRHxLnAncFqR4/4WuB54J8NYrEENZubTcvMcuadgtrssE8E44IVe2z1p2w6SPgwcHBE/yTAOa3BdXbB2bTJH0dq1yXapu48WLCi9HoLvQDIrrmZ3DUkaAXwD+NIAjp0jqVtS98aNG7MPzupeuZlPPVbBrDKZ3TUk6Xjg6oj4VLp9JUBEXJtu7w/8G/Bm+pL3A68Ap0ZEyduCfNeQ9Wc470CCJFGsX58kmAULXGewxlSru4aWA5MlTZS0B3AWsKSwMyK2RMTYiOiIiA7gN/STBMwGYrjuQLrkEl9KsnzILBFExFbgC8D9wNPA3RGxWtLXJZ2a1eeaQWV1hVKjmjdv9qUky4dMawQRsTQiDomIP46IBWnb1yJiSZFjp7s3YFmqtKdQSn9FZycJazSZjSMwq0eVjFXYa6+kV9BXf4PZSo1hANcbrD55igkzkm/tff9IQ2VFZyl57bp1u+9rbYW333ZB2mqnXLHYicCsjGIJYv784n/sJ0xIjqvkf6lyCcLJwIaT1yMwG6RKB7OVGsNQigvSVg+cCMwqVG4wW6kk0dpa2We4IG3V5GKx2SCUKjoX2gZab3BB2uqBawRmVeKCtNWSi8VmdayWBWlwgsgLF4vN6litCtLlptAoV4NwfaIJRURDPY455pgwy4M77oiYMCFCSn7eccfO9jFjIpI/38ljzJiI1tZd2wb7aG0t/v533FH6swv7isVb6flZNoDuKPF31ZeGzBpQJfWGUgXpShWm4hiO+kSpWH25KjuuEZjlRJYJQkp+Dkd9otRn9zfArtj5OUkMTLlEUPNLPZU+fGnIrHLFLsNUeolpwoTkMRyXnyp9FGKu9LKULz/thC8NmVkxlfQgCpdtsrz8VMpgbps95xy4/XZffipwj8DMKlLum/Rw9C5KFaTL9UakynoRLS2VfXZ/vYhG73VQpkdQ8z/slT6cCMzqUyUJotTlnHLHZ31ZqtzlpwsvrKx9sJerskwqTgRmVjPDdVtppb2OUj2CUo/C51XyXpX2OvpLHMN1a24x5RKBawRm1jAqqWmUqhGUqmcMZtR2pVpaii+NOthbcyupa3hksZk1hWKjsEvNBnvTTcXbv/Wtykdtt7RU1l5KqfWx169PHsWUm6p8uHj2UTNreOVmgy31rbnUXUOV9C4q7XWU6hEUElCxHkEppRLHYGTaI5A0U9Kzkp6TdEWR/X8t6SlJT0p6UNKELOMxM4PiPYtCeyW9i0p7HXPmlO6NVLqWRaVzTpVVqngw1AfQAvwbMAnYA1gJHNHnmBnAmPT5hcBd/b2vi8Vm1giG666hckXkSlCLYrGk44GrI+JT6faVaeK5tsTxRwPfjogTy72vi8VmljfDMbVGuWJxljWCccALvbZ7gOPKHH8e8NNiOyTNAeYAjB/W/pCZWf0rV+sYDnVx15Cks4FO4O+K7Y+IhRHRGRGdbW1t1Q3OzKzJZdkjeBE4uNd2e9q2C0kfB+YDJ0XEf2YYj5mZFZFlj2A5MFnSREl7AGcBS3ofkNYF/gE4NSI2ZBiLmZmVkFkiiIitwBeA+4GngbsjYrWkr0s6NT3s74B9gO9LekLSkhJvZ2ZmGcl0QFlELAWW9mn7Wq/nH8/y883MrH8NN9eQpI1ABePvdjEW2DSM4TSSvJ67zztffN6lTYiIonfbNFwiGApJ3aXuo212eT13n3e++LwHpy5uHzUzs9pxIjAzy7m8JYKFtQ6ghvJ67j7vfPF5D0KuagRmZra7vPUIzMysDycCM7Ocy00i6G+RnGYh6VZJGySt6tX2R5J+Jun/pT8PrGWMWZB0sKSH0oWOVku6JG1v6nOXNFrSo5JWpuf9N2n7REm/TX/f70qneWk6klokPS7px+l205+3pLWSfpfOxtCdtg3p9zwXiUBSC3Aj8GngCGC2pCNqG1VmbgNm9mm7AngwIiYDD6bbzWYr8KWIOAL4CHBR+m/c7Of+n8DHIuJDwFRgpqSPANcD34yIPwFeJZnmvRldQjKFTUFezntGREztNXZgSL/nuUgEwDTguYhYExHvAncCp9U4pkxExDLglT7NpwG3p89vBz5T1aCqICJeiojH0udvkPxxGEeTn3u6+NSb6eao9BHAx4D/k7Y33XkDSGoHZgHfTbdFDs67hCH9nuclERRbJGdcjWKphfdFxEvp838H3lfLYLImqQM4GvgtOTj39PLIE8AG4GckS8S+lk78CM37+/73wGXA9nS7lXycdwAPSFqRLtoFQ/w9z3TSOas/ERGSmvaeYUn7APcAX4yI15MviYlmPfeI2AZMlXQA8EPgsBqHlDlJpwAbImKFpOm1jqfKPhoRL0o6CPiZpGd67xzM73leegQDWiSnib0s6QMA6c+mXPtB0iiSJLAoIn6QNufi3AEi4jXgIeB44ABJhS96zfj7fiJwqqS1JJd6PwZ8i+Y/byLixfTnBpLEP40h/p7nJRH0u0hOk1sCnJM+Pwf4vzWMJRPp9eF/BJ6OiG/02tXU5y6pLe0JIGkv4BMk9ZGHgDPSw5ruvCPiyohoj4gOkv+f/zUiumjy85a0t6R9C8+BTwKrGOLveW5GFks6meSaYgtwa0QsqHFImZC0GJhOMi3ty8BVwL3A3cB4kim8PxsRfQvKDU3SR4FfAr9j5zXjr5DUCZr23CUdRVIcbCH5Ynd3RHxd0iSSb8p/BDwOnN2sS8Gml4b+R0Sc0uznnZ7fD9PNkcC/RMQCSa0M4fc8N4nAzMyKy8ulITMzK8GJwMws55wIzMxyzonAzCznnAjMzHLOicAsJWlbOqNj4TFsE9RJ6ug9I6xZPfEUE2Y7vR0RU2sdhFm1uUdg1o90/vf/mc4B/6ikP0nbOyT9q6QnJT0oaXza/j5JP0zXCFgp6YT0rVok3ZKuG/BAOhIYSfPSdRSelHRnjU7TcsyJwGynvfpcGvqzXvu2RMSRwLdJRqgD/G/g9og4ClgE3JC23wA8nK4R8GFgddo+GbgxIv4UeA04PW2/Ajg6fZ+5WZ2cWSkeWWyWkvRmROxTpH0tyeIva9KJ7f49IlolbQI+EBHvpe0vRcRYSRuB9t5TG6RTY/8sXTgESZcDoyLiGkn3AW+STAVyb6/1Bcyqwj0Cs4GJEs8r0XvOm23srNHNIllB78PA8l6zZ5pVhROB2cD8Wa+fv06fP0Iy8yVAF8mkd5AsFXgh7Fg0Zv9SbyppBHBwRDwEXA7sD+zWKzHLkr95mO20V7rSV8F9EVG4hfRASU+SfKufnbZdDPyTpC8DG4HPpe2XAAslnUfyzf9C4CWKawHuSJOFgBvSdQXMqsY1ArN+pDWCzojYVOtYzLLgS0NmZjnnHoGZWc65R2BmlnNOBGZmOedEYGaWc04EZmY550RgZpZz/x/AJuMjNqYQwAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 23s 2ms/step - loss: 1.2535 - accuracy: 0.6910\n",
            "Test accuracy = 0.6909999847412109\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "colab": {
      "name": "Assignment 2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}